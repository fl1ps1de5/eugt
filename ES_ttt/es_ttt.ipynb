{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TicTacToe ES Implementation\n",
    "\n",
    "Code for the environment, and inspiration was sourced from https://github.com/Zeta36/pytorch-es-tic-tac-toe/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class simpleMLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(simpleMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.linear1(x))\n",
    "        x = nn.functional.relu(self.linear2(x))\n",
    "        o = self.out(x)\n",
    "        return o\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        count = 0\n",
    "        for param in self.parameters():\n",
    "            count += param.data.numpy().flatten().shape[0]\n",
    "        return count\n",
    "\n",
    "    def es_params(self):\n",
    "        \"\"\"\n",
    "        The params that should be trained by ES (all of them)\n",
    "        \"\"\"\n",
    "        return [(k, v) for k, v in zip(self.state_dict().keys(),\n",
    "                                       self.state_dict().values())]\n",
    "    \n",
    "class ES(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        \"\"\"\n",
    "        Really I should be using inheritance for the small_net here\n",
    "        \"\"\"\n",
    "        super(ES, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, 256)\n",
    "        self.linear2 = nn.Linear(256, 256)\n",
    "        self.linear3 = nn.Linear(256, 256)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = F.elu(self.linear1(inputs))\n",
    "        x = F.elu(self.linear2(x))\n",
    "        x = F.elu(self.linear3(x))\n",
    "\n",
    "        return self.actor_linear(x)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        count = 0\n",
    "        for param in self.parameters():\n",
    "            count += param.data.numpy().flatten().shape[0]\n",
    "        return count\n",
    "\n",
    "    def es_params(self):\n",
    "        \"\"\"\n",
    "        The params that should be trained by ES (all of them)\n",
    "        \"\"\"\n",
    "        return [(k, v) for k, v in zip(self.state_dict().keys(),\n",
    "                                        self.state_dict().values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:113: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:113: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:116: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:119: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:121: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:144: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:150: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:162: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:113: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:113: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:116: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:119: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:121: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:144: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:150: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:162: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_13682/744860171.py:113: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(set(board[i * 3:i * 3 + 3])) is 1 and board[i * 3] is not '-': return True\n",
      "/tmp/ipykernel_13682/744860171.py:113: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if len(set(board[i * 3:i * 3 + 3])) is 1 and board[i * 3] is not '-': return True\n",
      "/tmp/ipykernel_13682/744860171.py:116: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if (board[i] is board[i + 3]) and (board[i] is board[i + 6]) and board[i] is not '-':\n",
      "/tmp/ipykernel_13682/744860171.py:119: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if board[0] is board[4] and board[4] is board[8] and board[4] is not '-':\n",
      "/tmp/ipykernel_13682/744860171.py:121: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if board[2] is board[4] and board[4] is board[6] and board[4] is not '-':\n",
      "/tmp/ipykernel_13682/744860171.py:144: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if player is 'X':\n",
      "/tmp/ipykernel_13682/744860171.py:150: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if c is 0:\n",
      "/tmp/ipykernel_13682/744860171.py:162: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if player is 'X':\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class TicTacToeEnv:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observation_space = 9\n",
    "        self.board = list('---------')\n",
    "        self.action_space = 9\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Given an action, the model moves and we observe the new state and return the reward .\n",
    "        Arguments:\n",
    "              action: a integer between 0 and 8 (the place to move).\n",
    "        Return Value:\n",
    "               new state of board, reward, game ended (True|False)\n",
    "        \"\"\"\n",
    "        if self.board[action] != '-':\n",
    "            # Ilegal move\n",
    "            return self.getBoard(), -0.2, True\n",
    "\n",
    "        #Model moves\n",
    "        self.board[action] = 'X'\n",
    "\n",
    "        #Is Draw\n",
    "        if self.isDraw(self.board):\n",
    "            return self.getBoard(), 1, True\n",
    "\n",
    "        #Classic AI moves\n",
    "        reward, move = self.nextMove(self.board, 'O')\n",
    "        self.board[move] = 'O'\n",
    "\n",
    "        # Classic AI wins\n",
    "        if self.isWin(self.board):\n",
    "            return self.getBoard(), -1, True\n",
    "\n",
    "        #Is Draw\n",
    "        if self.isDraw(self.board):\n",
    "            return self.getBoard(), 1, True\n",
    "\n",
    "        return self.getBoard(), 0.1, False\n",
    "\n",
    "    def getBoard(self):\n",
    "        board = np.asarray([ord(i) for i in self.board], np.float32)\n",
    "        return board\n",
    "\n",
    "    def clearProb(self, prob):\n",
    "        for i in range(9):\n",
    "            if self.board[i] != '-':\n",
    "                prob[i] = 0.0\n",
    "        return prob\n",
    "\n",
    "    def chunks(slef, l, n):\n",
    "        return [l[i:i + n] for i in range(0, len(l), n)]\n",
    "\n",
    "    def reset(self, first):\n",
    "        \"\"\"\n",
    "        Start a new game where first move 'first'\n",
    "        Return: the state of the reset board\n",
    "        \"\"\"\n",
    "        self.board = list('---------')\n",
    "        if first == 0:\n",
    "            p = random.random()\n",
    "            if p < 0.5:\n",
    "                _, move = self.nextMove(self.board, 'O')\n",
    "                self.board[move] = 'O'\n",
    "                print(\"\\nClassical AI moves first with 'O':\\n\")\n",
    "                return self.getBoard()\n",
    "            else:\n",
    "                print(\"\\nModel moves first with 'X':\\n\")\n",
    "                return self.getBoard()\n",
    "        elif first == 1:\n",
    "            return self.getBoard()\n",
    "        else:\n",
    "            _, move = self.nextMove(self.board, 'O')\n",
    "            self.board[move] = 'O'\n",
    "            return self.getBoard()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Print the board state\n",
    "        \"\"\"\n",
    "        board2D = np.array(self.chunks(self.board, 3))\n",
    "        for i in range(3):\n",
    "                print(board2D[i].flatten())\n",
    "\n",
    "        print('\\nX = Model AI, O = Classical AI\\n')\n",
    "\n",
    "    def isDraw(self, board):\n",
    "        \"\"\"\n",
    "        Was the game a draw?        \n",
    "        Return: True|False\n",
    "        \"\"\"\n",
    "        for i in range(9):\n",
    "            if board[i] == '-':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Taken from https://gist.github.com/SudhagarS/3942029\n",
    "    def isWin(self, board):\n",
    "        \"\"\"\n",
    "        Given a board checks if it is in a winning state.\n",
    "        Arguments:\n",
    "              board: a list containing X,O or -.\n",
    "        Return Value:\n",
    "               True if board in winning state. Else False\n",
    "        \"\"\"\n",
    "        ### check if any of the rows has winning combination\n",
    "        for i in range(3):\n",
    "            if len(set(board[i * 3:i * 3 + 3])) is 1 and board[i * 3] is not '-': return True\n",
    "        ### check if any of the Columns has winning combination\n",
    "        for i in range(3):\n",
    "            if (board[i] is board[i + 3]) and (board[i] is board[i + 6]) and board[i] is not '-':\n",
    "                return True\n",
    "        ### 2,4,6 and 0,4,8 cases\n",
    "        if board[0] is board[4] and board[4] is board[8] and board[4] is not '-':\n",
    "            return True\n",
    "        if board[2] is board[4] and board[4] is board[6] and board[4] is not '-':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def nextMove(self, board, player):\n",
    "        \"\"\"\n",
    "        Computes the next move for a player given the current board state and also\n",
    "        computes if the player will win or not.\n",
    "        Arguments:\n",
    "            board: list containing X,- and O\n",
    "            player: one character string 'X' or 'O'\n",
    "        Return Value:\n",
    "            willwin: 1 if 'X' is in winning state, 0 if the game is draw and -1 if 'O' is\n",
    "                        winning\n",
    "            nextmove: position where the player can play the next move so that the\n",
    "                             player wins or draws or delays the loss\n",
    "        \"\"\"\n",
    "        ### when board is '---------' evaluating next move takes some time since\n",
    "        ### the tree has 9! nodes. But it is clear in that state, the result is a draw\n",
    "        if len(set(board)) == 1: return 0, 4\n",
    "\n",
    "        nextplayer = 'X' if player == 'O' else 'O'\n",
    "        if self.isWin(board):\n",
    "            if player is 'X':\n",
    "                return -1, -1\n",
    "            else:\n",
    "                return 1, -1\n",
    "        res_list = []  # list for appending the result\n",
    "        c = board.count('-')\n",
    "        if c is 0:\n",
    "            return 0, -1\n",
    "        _list = []  # list for storing the indexes where '-' appears\n",
    "        for i in range(len(board)):\n",
    "            if board[i] == '-':\n",
    "                _list.append(i)\n",
    "        # tempboardlist=list(board)\n",
    "        for i in _list:\n",
    "            board[i] = player\n",
    "            ret, move = self.nextMove(board, nextplayer)\n",
    "            res_list.append(ret)\n",
    "            board[i] = '-'\n",
    "        if player is 'X':\n",
    "            maxele = max(res_list)\n",
    "            return maxele, _list[res_list.index(maxele)]\n",
    "        else:\n",
    "            minele = min(res_list)\n",
    "            return minele, _list[res_list.index(minele)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components we need are:\n",
    "\n",
    "* Function to add noise to the weights of the model\n",
    "    * **Input**: Neural network/actor\n",
    "    * **Output**: Array (population) of weights with noise added \n",
    "    * Key part of ES, noise is added to model weights to create a population (represents exploration)\n",
    "\n",
    "* Function to evaluate an agent by running it in the environment and computing total reward\n",
    "    * **Input**: Neural network/actor , environment\n",
    "    * **Output**: Total reward, calculated over certain episode length \n",
    "    * Finds the action, based on the NN \n",
    "    * Determines reward and a new observation state based on stepping through the environment\n",
    "    * Sums the reward after each step into a variable (total_reward)\n",
    "\n",
    "* Function to evaluate the \"noisy\" agent by adding noise to the \"plain\" agent and then using the prior (^) function\n",
    "    * **Input**: Noise generated by noise function , neural network , environment\n",
    "    * **Output**: Reward for specific \"noisy\" NN\n",
    "\n",
    "* Rollout/worker function\n",
    "    * **Input**: params_queue (a global unit to store params) , output_queue (a global unit to store outputs)\n",
    "    * As we are working for multiple processes\n",
    "        * Each worker gets the agent NN, the sample noise, and evalutates the agent (adding the noise)\n",
    "    * The env is created\n",
    "    * The actor is determined (NN is created)\n",
    "    * Then loop, and if the params_queue is not empty\n",
    "        * Load the model with params from params_queue\n",
    "        * Get a new random seed and set the new seed\n",
    "        * Get noise from the noise function\n",
    "        * Calculate reward from prior (^) function\n",
    "        * Add this reward to output_queue (and the seed?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "\n",
    "def generate_noise(neural_net):\n",
    "    nn_noise = []\n",
    "    for n in neural_net.parameters():\n",
    "        noise = np.random.normal(size=n.data.numpy().shape)\n",
    "        nn_noise.append(noise)\n",
    "    return np.array(nn_noise)\n",
    "\n",
    "\n",
    "def evaluate_NN(nn, env):\n",
    "    obs = env.reset(1)\n",
    "    game_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # neural net output\n",
    "        net_output = nn(torch.tensor(obs))\n",
    "        # get action with max liklihood\n",
    "        action = net_output.data.cpu().numpy().argmax()\n",
    "        new_obs, reward, done = env.step(action)\n",
    "        obs = new_obs\n",
    "\n",
    "        game_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return game_reward\n",
    "\n",
    "def evaluate_noisy_NN(noise, nn, env):\n",
    "    print(\"noisy going\")\n",
    "    \n",
    "    # save noise free params\n",
    "    old_dict = nn.state_dict()\n",
    "\n",
    "    # add noise to params\n",
    "    for n, p in zip(noise, nn.parameters()):\n",
    "        p.data += torch.FloatTensor(n * NOISE_STD)\n",
    "\n",
    "    reward = evaluate_NN(nn, env)\n",
    "\n",
    "    # load previous parameters (with no noise)\n",
    "    nn.load_state_dict(old_dict)\n",
    "\n",
    "    return reward\n",
    "\n",
    "def worker(param_queue, output_queue):\n",
    "    print(\"worker started\")\n",
    "\n",
    "    env = TicTacToeEnv()\n",
    "    actor = simpleMLP(env.observation_space, env.action_space)\n",
    "    \n",
    "    while True:\n",
    "        actor_params = param_queue.get()\n",
    "        print(\"HELLO\")\n",
    "        print(f\"Actor params: {actor_params}\")\n",
    "        if actor_params != None:\n",
    "            print(\"Worker loading parameters...\")\n",
    "            # load actor params from queue\n",
    "            actor.load_state_dict(actor_params)\n",
    "\n",
    "            # get random seed\n",
    "            seed = np.random.randint(1e5)\n",
    "            \n",
    "            # set new seed\n",
    "            np.random.seed(seed)\n",
    "            print(f\"Worker using seed {seed}.\")\n",
    "\n",
    "            noise = generate_noise(actor)\n",
    "\n",
    "            reward = evaluate_noisy_NN(noise, actor, env)\n",
    "\n",
    "            print(f\"Worker completed evaluation with reward {reward}.\")\n",
    "\n",
    "            output_queue.put([reward, seed])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def normalized_rank(rewards):\n",
    "    ranked = ss.rankdata(rewards)\n",
    "    norm = (ranked - 1) / (len(ranked) - 1)\n",
    "    norm -= 0.5\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker started\n",
      "workers started!\n",
      "Iteration 0 - collecting rewards...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# get results from each worker\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(POPULATIONSIZE):\n\u001b[0;32m---> 44\u001b[0m     r, s \u001b[38;5;241m=\u001b[39m \u001b[43moutput_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(s)\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.8/multiprocessing/queues.py:97\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m---> 97\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.8/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/thesis/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch import optim\n",
    "import time\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Training on:\", device)\n",
    "\n",
    "NOISE_STD = 0.05\n",
    "POPULATIONSIZE = 10\n",
    "LEARNINGRATE = 0.01\n",
    "MAXITERATIONS = 100\n",
    "\n",
    "WORKERS = 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = TicTacToeEnv()\n",
    "\n",
    "    actor = simpleMLP(env.observation_space, env.action_space)\n",
    "    optimizer = optim.Adam(actor.parameters(), lr = LEARNINGRATE)\n",
    "\n",
    "    # create queues so I can pass variables between processes\n",
    "    output_queue = mp.Queue(maxsize=POPULATIONSIZE)\n",
    "    param_queue = mp.Queue(maxsize=POPULATIONSIZE)\n",
    "\n",
    "    processes = []\n",
    "\n",
    "    for _ in range(WORKERS):\n",
    "        p = mp.Process(target=worker, args=(param_queue, output_queue))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    print(\"workers started!\")\n",
    "\n",
    "    for iteration in range(MAXITERATIONS):\n",
    "        it_time = time.time()\n",
    "\n",
    "        batch_noise = []\n",
    "        batch_reward = []\n",
    "\n",
    "\n",
    "        print(f\"Iteration {iteration} - collecting rewards...\")\n",
    "        # get results from each worker\n",
    "        for _ in range(POPULATIONSIZE):\n",
    "            r, s = output_queue.get()\n",
    "\n",
    "            print(f\"Received reward: {r} with seed {s}\")\n",
    "\n",
    "            np.random.seed(s)\n",
    "            noise = generate_noise(actor)\n",
    "            batch_noise.append(noise)\n",
    "\n",
    "            batch_reward.append(r)\n",
    "        \n",
    "        print(\"population created\")\n",
    "\n",
    "        avg_reward = np.round(np.mean(batch_reward), 2)\n",
    "        print(iteration, 'Mean:',avg_reward, 'Max:', np.round(np.max(batch_reward), 2), 'Time:', np.round(time.time()-it_time, 2)) \n",
    "\n",
    "\n",
    "        th_update = []\n",
    "        optimizer.zero_grad()\n",
    "        # for each actor's parameter, and for each noise in the batch, update it by the reward * the noise value\n",
    "        for idx, p in enumerate(actor.parameters()):\n",
    "            upd_weights = np.zeros(p.data.shape)\n",
    "\n",
    "            for n,r in zip(batch_noise, batch_reward):\n",
    "                upd_weights += r*n[idx]\n",
    "\n",
    "            upd_weights = upd_weights / (POPULATIONSIZE*NOISE_STD)\n",
    "            # put the updated weight on the gradient variable so that afterwards the optimizer will use it\n",
    "            p.grad = torch.FloatTensor( -upd_weights)\n",
    "            th_update.append(np.mean(upd_weights))\n",
    "\n",
    "        # Optimize the actor's NN\n",
    "        optimizer.step()\n",
    "\n",
    "    # quit processes\n",
    "\n",
    "    for _ in range(WORKERS):\n",
    "        param_queue.put(None)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Process name='Process-40' pid=12313 parent=10013 started>\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
